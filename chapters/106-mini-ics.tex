\chapter{简单理解计算机系统}

\begin{outline}
  本章介绍了计算机系统的基本原理，涵盖了信息的表示、整数和浮点数的存储方式、内存地址的概念以及字符编码和字体的基础知识。通过学习本章内容，读者将能够更好地理解计算机如何处理和存储数据，为后续的编程和计算机科学学习打下坚实的基础。
\end{outline}

\begin{prerequisite}
  \begin{itemize}
    \item 计算机的基本组成部分（CPU、内存、存储设备等）
  \end{itemize}
\end{prerequisite}

\begin{flushright}
本章节蒸馏自CMU与PKU的ICS课程，内容经过了大量的删减和改编，但尽可能地保留了原有的精华。
\end{flushright}

我在写第十章《用GDB调试》的时候，和LCPU的同学们在群聊里面吹了吹水。然后，我发现一个非常严重的问题：大多数人对于汇编、CPU、内存等概念都不甚了解，结果我写出来的一大堆东西全是鸡同鸭讲，堪比天书，这样的东西完全违背了我的本意。因此，我紧急中断了第十章的编写，决定先写一个mini ics，来介绍一下计算机的许多基本原理。

\textbf{我非常建议有时间的同学们阅读CSAPP这本经典教材，而且能读英文原版就读原版。}这是一本非常好的教材，内容非常全面，值得一读。我这一章是蒸馏版，把CSAPP的内容以及操作系统、计算机体系结构等相关课程的内容，浓缩成了这些问题，并比原版教材更像“讲故事”，适合零基础的同学入门。当然，虽然一个小时就能看完、立刻就能用，但是毕竟是缺少了一大堆细节的。

\section{信息怎么被表示？}

我们写的程序中，有各种各样的数据类型，例如整数、浮点数、字符、字符串等。计算机也只认识0和1，那么，这些繁多的数据类型在计算机中是怎么被表示的呢？

在计算机上，信息是以二进制的形式被表示的，也就是0和1。每一个数字都是一个二进制位（bit），8个二进制位组成一个字节（byte）。于是我们就了解了：计算机上的数字只是约定好的0-1串，且要遵从一定的格式；计算机本身并不知道内存中这里存的是什么，这一切实际上都是程序告诉计算机的。于是你就理解了为什么在C系语言中的 \texttt{union} 可以把同一块内存当成不同类型来读写：内存中的0-1串并没有类型，类型只是程序员的约定。

为了表示方便，计算机上一般利用十六进制的两位来表示一个字节，例如十进制下的11，写成二进制是0b1011，写成十六进制是0xB；其中 \texttt{0b} 表示后面这个是二进制数\footnote{仅限于GNU语法}， \texttt{0x} 表示后面这个是16进制。有时候也可以看见这个数在八进制下的表示013，这个开头0就表示后面这个是8进制数。

\subsection{整数}
整数在计算机中通常使用补码的形式来表示。对于一个n位（这个n指的是二进制位）的有符号整数，最高位是符号位，其他n-1位用于表示数值；对于无符号整数，所有n位都用于表示数值。

例如一个四位的有符号整数，实际数值是各位数值相加，最高位表示$-2^3$，剩下的三位表示$2^2 + 2^1 + 2^0$，因此它的数值范围是$-8$（0b1000）到$7$（0b0111）。而一个四位的无符号整数，所有四位都表示数值，因此它的数值范围是$0$到$15$。

32位计算机中，常见的整数类型 \texttt{int} 指的通常是32位的整数（也就是4个字节）。于是我们就知道了，int类型的最大值是$2^{31}-1$，最小值是$-2^{31}$。而无符号整数类型 \texttt{unsigned int} （以下简写为 \texttt{uint} ）的最大值可以表示为$2^{32}-1$，最小值为0。在C/C++中，int的最大值有宏定义 \texttt{INT\_MAX} ，最小值有宏定义 \texttt{INT\_MIN} ，无符号整数的最大值有宏定义 \texttt{UINT\_MAX} 。

我们知道，十进制中的加法可能会产生进位。二进制加法也不例外，例如$1+1=10$。但是在刚刚的讲解中，我们发现对于 \texttt{uint} 类的变量，最长只有32位，那么如果两个变量相加，可能会超过32位，比如两个“1后面跟31个0”这样的整数相加，结果是1后面跟32个0，超过了32位，这个时候就会发生溢出（overflow）。我们一般不能依赖于溢出后的结果。

一般情况下，计算机对溢出行为的处理是取最低32位的结果。例如两个 \texttt{uint} 类型的变量相加，如果结果超过了32位，那么计算机往往会将结果的低32位作为最终结果返回。对于有符号整数（ \texttt{int} ），溢出结果按模$2^n$回绕；通常情况下也可以理解为将结果的低32位作为最终结果返回。（当然如果一开始就是两个更长的整数相加，例如两个64位的整数相加发生溢出时，就会返回低64位的结果。）

\subsection{浮点数}

浮点数是另一个表示实数的方式。浮点数在计算机中通常使用IEEE 754标准来表示。一个32位的浮点数（单精度）由三部分组成：符号位、指数位和尾数位。浮点数的本质是科学计数法的变种。
\begin{itemize}
  \item 符号位（1位）：表示数值的正负。
  \item 指数位（8位）：表示数值的指数部分。
  \item 尾数位（23位）：表示数值的有效数字部分，存储的是隐含前导1的一个二进制小数，实际值是1.xxx。
\end{itemize}

具体说来，一个32位浮点数的数值可以表示为：
$$(-1)^{\text{符号位}} \times (1 + \text{尾数}) \times 2^{\text{指数} - 127}$$
其中，指数的偏移量是127，这个127的来源是$2^{(8-1)} - 1$。同理，一个64位的浮点数（双精度）由1位符号位、11位指数位和52位尾数位组成，指数的偏移量是1023。满足上述标准的单精度和双精度浮点数一般被称为规格化浮点数。因此，我们可以知道，最大的规格化单精度浮点数大概是$3.4 \times 10^{38}$，最小的规格化单精度浮点数大概是$1.2 \times 10^{-38}$；而最大的规格化双精度浮点数大概是$1.8 \times 10^{308}$，最小的规格化双精度浮点数大概是$2.2 \times 10^{-308}$。更精确的长数值（例如128位浮点数）也有类似的表示方法，这里就不赘述了。

现在的问题就变成，尾数位如果全是0，指数位如果全是0或者全是1，这种情况怎么办？IEEE 754标准对这些特殊情况做了规定：
\begin{itemize}
  \item 如果指数位全是0，尾数位全是0，那么表示的数值是0。
  \item 如果指数位全是0，尾数位不全是0，那么表示的数值是\textbf{非规格化数}，用于表示非常接近0的数值。
  \item 如果指数位全是1，尾数位全是0，那么表示的数值是无穷大（Infinity）。
  \item 如果指数位全是1，尾数位不全是0，那么表示的数值是NaN（Not a Number），用于表示未定义或不可表示的数值，例如0除以0。
\end{itemize}
对于32位非规格化浮点数，数值可以表示为：
$$(-1)^{\text{符号位}} \times (0 + \text{尾数}) \times 2^{-126}$$
32位非规格化数的指数部分被固定为-126。同理，64位非规格化浮点数的指数部分被固定为-1022。因此最大的非规格化单精度浮点数大概是$1.7 \times 10^{-38}$，最小的非规格化单精度浮点数大概是$1.4 \times 10^{-45}$；而最大的非规格化双精度浮点数大概是$1.0 \times 10^{-307}$，最小的非规格化双精度浮点数大概是$5.0 \times 10^{-324}$。

然而，有些数值无法精确表示为二进制浮点数，例如十进制下的0.1在二进制下是一个无限循环小数，无法用有限的位数表示。因此，浮点数在计算机中往往只能\textbf{近似}表示某些实数，且在数非常大的时候精度会进一步降低。例如计算机实际上计算$0.1+0.2=0.3000\cdots 004$。另一方面，浮点数\textbf{并不连续}，从上述表示方法可以看出浮点数也有间隔。一般的，把1到下一个浮点数之间的间隔叫做\textbf{机器精度}，例如单精度浮点数的机器精度大概是$1.2 \times 10^{-7}$，双精度浮点数的机器精度大概是$2.2 \times 10^{-16}$。

另一方面，我们知道浮点数是科学记数法，其精度有限，在进行加法或乘法运算时不可避免地会导致舍入误差，而先丢失的精度可能会影响最终的结果，因此\textbf{浮点数的运算并不满足交换律和结合律}。例如$1.0 + 1.0 \times 10^{-7} - 1.0 \times 10^{-7} \neq 1.0 + (1.0 \times 10^{-7} - 1.0 \times 10^{-7})$。而在连续运算当中，这个舍入误差会累积，导致结果偏差越来越大。我们在实际操作中应当避免这种情况，例如可以将数值从小到大排序后再进行运算，并尽可能地减少数量级相差过大的数值运算，来减少累积误差。

作为Mini ICS，我们只需要知道浮点数有误差就可以了（这是因为十进制小数可能无法精确表示为二进制浮点数）。因此，工程上\textbf{不可以利用浮点数来进行货币运算}，除非使用高精度浮点数（例如 \texttt{decimal} 类）。一般的处理是把货币放大100倍（也就是把元变成分），然后用整数来表示和计算。

有不少算法依赖于浮点数有精度这一客观事实，例如最大流算法中Ford-Fulkerson算法在计算机上的有限终止性证明就利用了这一特性。

\subsection{地址}

在内存中，每一个字节都有一个唯一的地址。地址通常是一个无符号整数，表示该字节在内存中的位置。地址的大小取决于计算机的架构，例如32位计算机的地址是32位的，而64位计算机的地址理论上是64位的（实际上大部分64位计算机只使用了48位或57位地址空间）。因此前者最多能表示4GB的内存，而后者理论上最多能表示16EB的内存。

有了地址，我们就可以对内存进行读写操作，也可以使用一些更高级的数据结构，例如数组、链表、树等。

\subsection{字符}\label{sec:locale}

我们知道，现在的计算机是使用二进制来存储数据的。但是，对于人类使用的语言而言，即使是相对简单的英语，也有52个大小写字母、10个数字、各种标点符号和特殊符号等，远远超过了二进制的0和1两种状态；而计算机却能够正确地显示并处理这些字符。这究竟是怎么做到的呢？

\subsubsection{从编码到字符}

一个非常容易想到的办法就是，给每一个字符分配唯一的一个编号，然后再使用二进制来表示该编号。通过建立字符和编号之间的唯一映射关系，我们就可以使用二进制来表示字符了。如果把许多个字符和编号之间的映射关系放在一起，就叫做\textbf{字符集}；至于如何建立字符和编号之间的映射关系，就叫做\textbf{编码方案}。

然而，一开始，不同厂家、不同地区等生产的计算机系统使用不同的编码方案，导致同一个编码在不同的系统上表现为不同的字符，对数据交换造成了严重的障碍。我们童年时候玩的新新魔塔就是一个非常典型的例子，其标题在中国的计算机上显示为“穝穝臸娥”，其中角色“暗黑大法师”被显示为“穞堵臸猭畍”，现在这个甚至成为了一个梗。

为了解决这个问题，也容易想到两种手段：要么利用一些方法来区分不同的编码方案，并在使用时指定编码方案；要么制定一个统一的编码方案，所有的计算机系统都使用这个编码方案。然而，前者的问题非常明显：如果这么做，那么所有的计算机都要存储所有的编码方案，这非常浪费空间；而且，如果用户不知道文件使用了哪种编码方案，那么就无法正确地显示文件内容。那么制定一个统一的编码方案反而成了一个不错的选择。这或许也是新时代的“书同文”吧。

最早的统一编码方案是美国人制定的ASCII编码。该编码使用7位二进制来表示128个字符，包含了英文字母、数字、标点符号和一些控制字符。例如，字母A的ASCII编码是65，字母a的ASCII编码是97，数字0的ASCII编码是48。可是世界上并非只有英语一种语言。为了兼容法语、德语等有变音符号的语言，后来又出现了ISO-8859（以Latin-1为代表）、扩展ASCII编码等，这些编码支持更多字符。

然而，当我们把目光投向亚洲时，我们发现了新的困难：以汉语为代表的亚洲语言有着数万个甚至数十万个字符，显然超过了上述编码的范围。为了促进国际交流，世界人民最终制定了一个统一的字符集：Unicode，或“统一码”。Unicode使用16位二进制来表示65536个字符，包含了世界上所有主要语言的字符以及一些符号和表情符号，同时该编码方案也完全兼容ASCII编码和扩展ASCII编码，例如0的Unicode编码仍然是48。后来也出现了扩展Unicode编码等，可以表示更多的字符。Unicode编码的出现极大地促进了国际交流和信息共享。为了和不同的计算机相适应，Unicode编码也有多种不同的表示方式，例如UTF-8、UTF-16、UTF-32等。其中，UTF-8是最常用的表示方式，它使用1到4个字节来表示一个字符，比较节省空间。Linux和mac OS系统默认使用UTF-8编码。

如果利用错误的编码方案来读取文件，就会出现乱码的问题。以下是常见的一些乱码及其原因，我们在看到这类乱码时，可以根据其原因来判断文件使用了哪种编码方案，从而选择正确的编码方案来读取文件。

\begin{note}
  列表中“烫烫烫”等行和“锟斤拷”一同在网络上十分流行，但是和锟斤拷等不同。烫烫烫等和编码本身无关。
  
   \texttt{0xCC} 等内容实际上是MSVC编译器在分配内存空间时填充的内容， \texttt{0xCC} 是未分配且未赋初值的内存空间，而 \texttt{0xCD} 是已动态分配但未赋初值的内存空间， \texttt{0xDD} 是已动态分配且已释放但未清理的内存空间。如果试图访问这些内存区域并以字符串形式打到终端上，就会出现烫烫烫等内容。

  举个有趣的例子：
  \begin{itemize}
    \item 小明煮了20个饺子。当他试图吃到第21个时，喊出“烫烫烫”。
    \item 小明说“我要煮20个饺子”但是还没煮。当他试图吃饺子的时候，喊出“屯屯屯”。
    \item 小明煮了20个饺子，吃光了并洗了碗。当他试图吃洗完的碗里的饺子时，喊出“葺葺葺”。
  \end{itemize}
\end{note}

\begin{table}[ht]
  \centering
  \caption{常见乱码以及其可能的原因}
  \begin{tabular}{c|c}
    \toprule
    乱码 & 原因 \\
    \midrule
    锟斤拷 & GBK读UTF-8 \\
    大量非法字符和西欧字符 & UTF-8读GBK \\
    仅大量西欧字符，原文变长 & Latin1读UTF-8或GBK \\
    出现大量长得像yp的东西 & UTF-8读UTF-16 \\
    大量不认识的汉字 & GBK读Big-5 \\
    \midrule
    烫烫烫 & UTF-8的 \texttt{0xCC} \\
    屯屯屯 & UTF-8的 \texttt{0xCD} \\
    葺葺葺 & UTF-8的 \texttt{0xDD} \\
    \bottomrule
  \end{tabular}
\end{table}

由于一些历史遗留问题，在Windows系统中，如果使用中文系统，则其编码方案通常是GBK（国标扩）。然而，该编码方案并不兼容Unicode编码，因此在处理Unicode编码的文件时，可能会出现乱码的问题。为了解决这个问题，Windows系统后来也提供了Unicode编码的支持，虽然系统本身依然使用GBK编码，但是在一些应用程序中会默认使用Unicode编码，例如记事本、Word等，且这些应用程序往往会自动识别文件的编码方案并进行转换。然而，对于一些从Linux等系统中移植来的软件往往是使用Unicode而非GB2312编码的，在Windows系统中运行这些软件时，可能会出现乱码的问题，因此，我们如希望使用一些移植软件，应当避免使用中文，或者在Windows系统中使用UTF-8编码。

在Windows 10以及以后的系统中，我们可以通过 \texttt{设置>时间和语言>语言>管理语言设置>更改系统区域设置} ，来将系统的默认编码方案更改为UTF-8编码，从而避免乱码的问题。此类编码应在系统新安装时就启用，以保证不会出现乱码问题。

然而，电脑屏幕上的同一个汉字，往往也有着不同的表现形式。这又是怎么做到的？难不成，Unicode为同一个汉字的不同样子都分配了不同的编码？显然不是这样。实际上，Unicode只为每一个汉字分配了一个编码，而同一个汉字的不同表现形式，是通过不同的\textbf{字形}来实现的。简而言之，“字符”是它的身体，而“字形”则是它的外套。

\subsubsection{从字符到字形}

Unicode为每一个抽象字符都分配了一个唯一的编码，例如汉字“汉”的Unicode编码是U+6C49。但是这仅仅规定了这是什么字符，却并没有规定这个字符长什么样。真正的字符长相，是由\textbf{字形}来决定的。字形实际上是在计算机出现之前就已经存在的概念，指的是是字符的具体表现形式：横平竖直还是行云流水，撇是飘带还是刀刃，点是瓜子还是露珠，这些都由字形来决定。

同一个字符可以有着多种字形，这些字形可以有着不同的风格和特点。例如，宋体、黑体、楷体等都是汉字的不同字形；英语等字母语言也有不同的字形，例如Times New Roman、Arial、Courier New等。把目光放远到全世界，我们会发现同一个字符在不同的国家和地区也有着不同的字形，例如中国汉字和日本、韩国所用汉字的字形就有着明显的差异。但是它们都是同一个字符，只是字形不同而已——这便是大一统下的多样性。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{100/source.jpg}
  \caption{思源字体的不同字形示例}
\end{figure}

不同的字形可以给人不同的感觉和印象，例如宋体给人正式、庄重的感觉，而楷体则给人优雅、柔和的感觉。因此，正确使用字形可以提高文本的可读性和美观性。

\subsubsection{从字形到字体}

如果把一套风格相同的字形放在一起，装进一个索引文件，再配上一个索引表，就形成了\textbf{字体文件}，使用这个字体文件的字形集合就是我们常说的\textbf{字体}。例如，宋体字体文件中包含了宋体字形的集合，黑体字体文件中包含了黑体字形的集合。

当计算机试图显示一个字符的时候，它就会去字体文件中查找该字符对应的字形，然后将该字形显示出来。这个过程很容易让人想到活字印刷术：把一套字形雕刻在木块上，然后把这些木块放在一个盒子里，当需要显示某个字符的时候，就从盒子里取出对应的木块，然后将其印在纸上。而实际上一开始的确是这么做的，只是把纸张换成了屏幕罢了！

一开始，人们把字体中的每一个字形作为图片来存储，也就是所谓的“位图字体”。然而这样就会出现一个弊病：每一个字体的大小是固定的，如果我们想要一个大字体，简单地放大其图片是不行的：大家可以参考在手机上放大一张图片的效果，放大后的图片会变得模糊不清，甚至出现锯齿状的边缘。难道要为每一个字号做一个字体吗？显然不现实。

1978年，王选院士主持研制的汉字激光照排系统问世，标志着我国在汉字信息处理领域实现了从无到有的历史性突破。该系统采用了点阵字模技术，不仅可以生成不同字号的汉字，还数千倍地压缩了存储空间，极大地提高了汉字排版的效率和质量。简单地说，它突破了位图字体的局限性，正式的使得字形能够被“算出来”而不是“画出来”，这使得人们彻底打开了字体编写的思路。

1982年，Adobe推出划时代的字体格式PostScript，采用三次贝塞尔曲线来描述字形轮廓。这类字体被称为矢量字体，比王选院士的点阵字模技术更进一步。矢量字体可以通过数学公式来描述字形轮廓，因此可以任意缩放而不会失真，且文件体积较小，加载和渲染速度也较快。后来，苹果公司也推出了自己的矢量字体格式TrueType，采用二次贝塞尔曲线来描述字形轮廓，算起来更快。

之后，微软联手Adobe又搞出了OpenType字体，结合了PostScript和TrueType的优点，又能往里塞其他东西（字符变体、连字、小型大写文字、旧式数字等），成为了现在最常用的字体格式。举例说，我们现在看到的ff、fi等连字，实际上就是OpenType字体中的一个特性。汉字方面，王选院士为中文字体打下的坚实地基，也显著地促进了中文字体的不断发展。直至今日，计算机对汉字的处理方式依然是小字号点阵字模、大字号矢量字形的结合；现在，TrueType、OpenType成千上万种中文字体中数以亿计的汉字字形能够被我们随意使用，也离不开王选院士的开创性工作，王选院士也被誉为当代毕昇。

需要说明的是，PostScript等都是具体的\textbf{字体格式}，而非字体文件本身。目前的字体基本上都是TTF（TrueType Font）或OTF（OpenType Font）格式的字体文件，而PostScript字体文件（.ps）则很少见了（在Windows上，这个扩展名甚至让位于脚本文件！）。而宋体、黑体、Times New Roman、Arial等则是具体的\textbf{字体}，它们可以有不同的字体格式，例如宋体可以有TTF格式的宋体文件和OTF格式的宋体文件。实际上，常规使用者并不需要关系字体究竟是什么格式，只需要选择喜欢的字体（字形）即可。

\subsubsection{从字体到屏幕}

计算机屏幕是由无数个像素点组成的，每一个像素点可以显示一种颜色。当我们试图在屏幕上显示一个字符的时候，计算机会先去字体文件中查找该字符对应的字形，然后将该字形转换为一组像素点，并将这些像素点显示在屏幕上。这个过程叫做\textbf{字体渲染}。字体渲染的过程非常复杂，涉及到许多技术和算法，例如抗锯齿、次像素渲染、字距调整等。不同的操作系统和应用程序可能会使用不同的字体渲染引擎，导致同一个字体的同一个字符在不同的系统和应用程序中显示效果不同。

\subsubsection{字重、斜体和复合字体}

在计算机出现之前，字重和斜体等就随着着印刷术的发展而出现了。字重指的是字体的粗细程度，一般可以分为Regular（常规）、Bold（粗体）、Light（细体）等。斜体指的是字体是倾斜的，一般可以分为意大利体（也叫斜体，Italic）和倾斜体（Oblique）\footnote{意大利体指的是字形本身是倾斜的，而倾斜体指的是将常规矩形字体压扁成普通平行四边形而成的字体。}。字重和斜体可以用来强调文本中的某些部分，例如标题、关键词等，从而提高文本的可读性和美观性。

在设计字体的时候，我们自然会考虑设计不同的字重和斜体版本。然而，如果为每一个字体都设计一个不同字重的版本，而不同字重的版本有的还需要斜体，这样就会导致字体文件数量爆炸，且每一个字体文件的体积也会变得很大。为了解决这个问题，我们可以使用\textbf{复合字体}。复合字体是指将多个字重和样式的字体文件组合在一起，形成一个统一的字体文件，这样不仅便于分发，也便于管理和使用。复合字体通常会包含Regular、Bold、Italic、Bold Italic等常用的字重和样式。

然而，大多数汉字字体并没有斜体字体：这是因为汉字是方块字，斜着不好看，没有这方面的需求，因此大多数汉字字体并没有斜体版本。对于英文字体而言，斜体是非常常见的，例如Times New Roman、Arial等都有斜体版本。

那有的读者会问了：为什么在MS Office中，我们能对汉字进行倾斜操作呢？这是因为，微软对斜体的定义比较宽泛：只要是倾斜的都叫斜体，而不一定非得是斜体版本的字体。对于没有斜体版本的汉字字体，微软会通过软件算法来实现倾斜效果，形成所谓的“伪斜体”。类似的，微软的Word等软件也会对没有粗体版本的汉字字体进行加粗处理，形成所谓的“伪粗体”，例如微软宋体（宋体，SimSun）就没有粗体版本，微软会通过软件算法来实现加粗效果，形成伪粗体。而对于Times New Roman等有粗体和斜体版本的字体，微软则会直接使用对应版本的字体。所以我们会发现，Times New Roman等字体的斜体和整体比起来字形有明显的差异，而宋体汉字的斜体和整体比起来字形没有明显的差异。

\subsubsection{衬线、无衬线和等宽字体}

字体可以分为衬线字体、无衬线字体和等宽字体三种类型。

衬线字体（Serif）是指在字形的笔画末端有小装饰线条的字体，例如宋体、Times New Roman等。衬线字体通常被认为更适合用于印刷品和长篇文本，因为衬线可以引导读者的视线，提高文本的可读性。无衬线字体（Sans Serif）是指没有衬线的字体，例如黑体、Arial等。无衬线字体通常被认为更适合用于屏幕显示和短篇文本，因为无衬线字体更简洁、现代，且在低分辨率下也能保持清晰。等宽字体（Monospace）是指每一个字符占用相同宽度的字体，例如Courier New、Consolas等，也叫打字机字体。等宽字体通常被认为更适合用于编程和代码编辑，因为等宽字体可以使代码更整齐、易读，且便于对齐和排版。

我们知道，汉字是方块字，每一个字它天然就是等宽的，所以为汉字设计等宽字体没有什么意义，也就没有汉字的等宽字体。一般排版中，往往使用汉字的无衬线字体（例如黑体、微软雅黑等）来搭配英文字体的等宽字体（例如Consolas、Courier New等），以达到较好的视觉效果。有些时候用仿宋搭配西文等宽字体也不错。

\subsubsection{LF和CRLF}

刚刚讲完字符和编码，我们就可以来讲讲这个东西了。

LF和CRLF是两种不同的换行符表示方式。LF是Line Feed的缩写，表示换行符；CRLF是Carriage Return和Line Feed的组合，表示回车换行符。有的人可能会疑惑：现代计算机上，回车和换行不是一回事吗？为什么还要区分这两者呢？实际上，这个问题要追溯到打字机时代。

在打字机时代，回车和换行不是一个键。回车是指将打印头移动到行首（但行数不变），而换行是指将打印头移动到下一行（但列数不变）。这里能提“行列”的主要原因是，西文打字机的本质实际上是在表格中输入，其字符也是等宽的。所以我们在打字机上，如果想要达到现在计算机上的换行效果，就需要先按回车键将打印头移动到行首，然后再按换行键将打印头移动到下一行。

那为什么现代计算机上这两个键就没有了呢？其实归根到底完全可以用“麻烦”来解释，合二为一能够简化键盘。但是，在ASCII码表中，却保留了这两个控制字符：CR（Carriage Return，回车，ASCII码为13）和LF（Line Feed，换行，ASCII码为10），分别用\textbackslash r和\textbackslash n来表示。

而现代不同操作系统对这两个控制字符的使用方式却不一样：Unix和类Unix系统（如Linux、mac OS等）使用LF作为换行符，而Windows系统则使用CRLF作为换行符。实际上这是历史遗留问题：在上世纪，当时系统三巨头是Unix、Dos和Mac OS，它们分别使用LF、CRLF和CR（你没看错，Mac OS早期版本使用CR，没有LF）作为换行符。后来，Mac OS内核受到BSD Unix的启发，从古老的Classic Mac OS（使用CR作为换行符）演变成了mac OS X（基于Unix的系统），因此也顺便改用了Unix的LF。而Dos则演变成了Windows 3.1（当时Windows 3.1仅是一个运行在DOS系统上的软件！），并沿用了Dos的CRLF作为换行符；再后来，Windows NT内核（Windows 95/98混血，2000开始全面使用NT内核）诞生，但是为了兼容性，Windows依然沿用了CRLF作为换行符！

这就导致了一个问题：如果我们在Windows系统上创建了一个文本文件，然后将其复制到Linux系统上打开，可能会出现换行符显示异常的问题（行尾出现不可见字符）。同样地，如果我们在Linux系统上创建了一个文本文件，然后将其复制到Windows系统上打开，可能会出现换行符显示异常的问题（所有内容都在一行显示）。

那这就很麻烦了，有没有什么解决办法呢？当然有！大多数现代文本编辑器（如VS Code、Notepad++等）都支持自动识别和转换不同的换行符格式，我们只需要在保存文件时选择合适的换行符格式即可。此外，我们也可以使用一些命令行工具（如 \texttt{dos2unix} 和 \texttt{unix2dos} ）来转换文本文件的换行符格式。但令人不爽的是，Git会导致换行符格式混乱的问题，这需要我们在使用Git时特别注意，建议在Git配置中设置合适的换行符处理方式（例如 \texttt{core.autocrlf} 选项），即明确规定在检出和提交时如何处理换行符格式。

这也说明了，当今社会是一个统一的社会，标准的不统一依然是一个极其让人头疼的问题，至于之后究竟是全面统一成CRLF还是LF，有待于后人的努力了。

\section{程序怎么跑起来？}

不知道同学们在写程序的时候，会不会疑惑“为什么我写的代码只是文本文件，但为什么这些特定的文本文件能够变成一个可以执行的程序、而我随便写的文章等却不能”。而另一个可能疑惑的问题是“只认识二进制的计算机，为什么能看懂我写的语言”。这就涉及到程序的编译和链接过程了。

我们知道，计算机的编程语言有很多种，这些高级语言都是方便人类来编写的。因此，就需要一些工具来把这些高级语言翻译成计算机能看懂的机器码（machine code）。对于C系语言写出的程序，一般情况下是由源代码（.c或.cpp文件）编译成目标代码（.o或.obj文件），然后链接成可执行文件（.exe或.out文件）。这个过程通常分为三个步骤：预处理、编译和链接。

\subsection{预处理}
预处理是对源代码进行一些简单的文本替换和宏展开。预处理器会处理一些指令，例如 \texttt{\#include} 、 \texttt{\#define} 等。同时，预处理会除去源代码中的所有注释。从某种程度上来说，预处理后的代码和源代码完全等价。

\subsection{编译和汇编}
计算机通过编译器将预处理后的代码转换为汇编码（能读懂一部分），然后再利用汇编器把汇编码转变成机器码（二进制码，人几乎读不懂）。编译器会将源代码转换为目标代码（.o或.obj文件），这个目标代码是特定于处理器架构的。这两步原理和过程相近，因此我们把它们合并在一起，但实际上是两步，这一点需要注意。

编译器会将源代码中的每个函数、变量等转换为机器码指令，并生成符号表（symbol table）来记录这些符号的地址。

编译器还会进行一些优化，例如常量折叠、循环展开等，以提高程序的执行效率。默认情况下，gcc编译器会进行一些基本的优化，但如果需要更高的优化级别，可以使用 \texttt{-O2} 或 \texttt{-O3} 选项。\textbf{优化可能暴露代码中的未定义行为}，但是不会导致本符合标准的代码出现错误。因此，我们一定要尽可能地编写符合标准的代码。

\subsection{链接}
链接是将多个目标代码文件和库文件合并成一个基于特定操作系统的可执行文件的过程。链接器会将目标代码中的符号解析为实际的地址，并将它们合并成一个完整的可执行文件。链接也是一个比较复杂的过程，在这里我们不深入讨论。

\subsection{常见汇编码}

x86-64架构是Intel的64位架构，在现代计算机非常常见。我们会利用该架构来简单解释汇编码的语法。

在深入介绍汇编码之前，我们要先了解一下CPU的寄存器。寄存器是CPU内部的高速存储器，用于存储临时数据和指令。x86-64架构有16个通用寄存器（RAX、RBX、RCX、RDX、RSI、RDI、RBP、RSP、R8-R15），每个寄存器都是64位的。CPU将指令和数据加载到寄存器中，利用控制单元CU来执行指令，利用算术逻辑单元ALU来进行计算。

x86-64架构的汇编码通常由操作码（opcode）和操作数（operand）组成。操作码是指令的名称，操作数是指令的参数。以下是一些常见的汇编码指令：
\begin{itemize}
  \item  \texttt{mov} ：将数据从一个寄存器或内存位置移动到另一个寄存器或内存位置。
  \item  \texttt{add} ：将两个寄存器或内存位置的值相加，并将结果存储在第一个寄存器或内存位置中。
  \item  \texttt{sub} ：将一个寄存器或内存位置的值减去另一个寄存器或内存位置的值，并将结果存储在第一个寄存器或内存位置中。
  \item  \texttt{jmp} ：无条件跳转到指定的标签。
  \item  \texttt{call} ：调用函数，将返回地址压入栈中。
  \item  \texttt{ret} ：从函数返回，弹出栈顶的返回地址。
  \item  \texttt{cmp} ：比较两个寄存器或内存位置的值，并设置标志位。
  \item  \texttt{je} 、 \texttt{jne} 、 \texttt{jg} 、 \texttt{jl} 等：条件跳转指令，根据比较结果跳转到指定的标签。
\end{itemize}

在x86-64架构中，假如我们想要把某个整数从寄存器RAX移动到寄存器RBX，可以使用以下指令：
\begin{lstlisting}
mov rbx, rax
\end{lstlisting}
或者说我们想要call一个函数，假设函数名为 \texttt{foo} ，可以使用以下指令：
\begin{lstlisting}
call foo
\end{lstlisting}
这是最基本的一些汇编码指令。对于其他的机器（例如Arm架构），汇编码的语法和指令可能会有所不同，但基本原理是相似的。编译器会在不同的架构上生成不同的汇编码，来保证程序的正确性和效率。

对于同一个机器，不同程序的同一句汇编码被编译出的机器码是一样的。比如说在x86-64架构上且使用AT\&T语法时， \texttt{mov rbx, rax} 这句汇编码被编译成的机器码永远 \texttt{48 89 D8} ，不会改变。

\subsection{其他情况}

对于解释性语言，情况略有不同。

以Python为例，它是一种解释性语言。这种语言并不需要先编译成机器码，而是通过解释器（例如CPython）先把 \texttt{*.py} 文件编译成\textbf{字节码}（bytecode）（.pyc文件），然后再由虚拟机（VM）来逐条解释执行这些字节码。字节码是一种中间表示形式，介于源代码和机器码之间。Python的字节码是与平台无关的，可以在任何支持Python解释器的系统上运行。而另一些工具（例如PyPy、Jython）会有JIT编译功能，能够把字节码编译成更高效的机器码来执行，而不是逐条解释执行。

而对于以C\#为首的“中间语言”，情况又略有区别。C\#是一种编译型语言，但它并不直接编译成机器码，而是编译成一种中间语言（Intermediate Language，IL），也叫做托管代码（Managed Code）。这种中间语言是一种与平台无关的字节码，可以在任何支持.NET框架的系统上运行。然后，.NET框架会利用即时编译器（JIT compiler）将中间语言编译成特定平台的机器码来执行。因此，C\#的逆向工程非常容易，直接反编译IL就能得到接近源代码的结果。

\subsection{从文件到程序}

刚才，我们知道了程序是怎么从源码转变成可执行文件的。那么，程序是怎么从可执行文件跑起来的呢？

在Linux中，可执行文件叫做ELF（Executable and Linkable Format）文件。ELF文件包含了程序的代码段（text segment）、数据段（data segment）、堆（heap）、栈（stack）等信息。操作系统通过加载器（loader）将ELF文件加载到内存中，并创建一个新的进程来执行该程序。在Windows中，可执行文件叫做PE（Portable Executable）文件，原理类似。

当我们运行一个可执行文件时，操作系统会将文件加载到内存中，并创建一个新的\textbf{进程}来执行该程序。每一个进程中都会有一个或多个\textbf{线程}，线程是进程中的一个执行单元。每个线程都有自己的寄存器状态和栈空间，但多个线程可以共享进程的内存空间。
操作系统会为该进程分配内存空间，并将程序的代码和数据加载到内存中。然后，操作系统会将CPU的控制权转移到程序的入口点（通常是 \texttt{main} 函数），开始执行程序。

程序在执行过程中，CPU会不断地从内存中取指令，并执行这些指令。程序可能会调用其他函数、分配和释放内存、进行输入输出等操作。当程序执行完毕后，操作系统会回收该进程的资源，并将控制权返回给操作系统。一般情况下，程序只能够访问分配给自己的内存空间，不能访问其他进程的内存空间。程序一般无法直接操作外存中的内容，必须通过操作系统提供的系统调用来进行文件读写等操作，显著地降低了恶意程序破坏文件的风险。

当然，也有一些病毒等恶意程序会利用系统漏洞来直接操作其他进程的内存空间，或者直接操作外存中的内容，破坏文件系统的完整性和安全性。

\section{计算机如何启动？}

我们从商店买来一台计算机，插上电源，按下电源键，计算机就启动了。这个过程发生了什么呢？硬件在这段时间内如何协同工作，使得计算机能够启动？为什么新买的计算机启动很快，而旧计算机启动很慢？为什么在更换一个性能更好的显卡之后，计算机反而启动变慢了？为了回答这些问题，我们需要了解计算机的启动过程。

计算机的启动过程可以分为以下几个步骤。

\subsection{加电自检}

实际上一个很反直觉的事情是，计算机在关机的时候也是一直需要通电的。计算机的主板上有一个小电池，叫做CMOS电池，它负责为计算机提供基本的电源；主板上还有一个芯片，叫做BIOS/UEFI芯片，它记录了当前时间、日期、硬件配置等信息。

当我们按下电源键时，主板收到“按下电源键”这个信号后，给电源发信号。于是电源率先启动，但它并不是立即给所有硬件供电，而是按严格的时间顺序先输出3.3V、5V、12V等不同的电压，自己也会不断检测这些电压是否在允许的误差值内。如果一切良好，电源会向主板发送一个“Power Good”信号，表示电源已经稳定工作。否则，主板会立即切断电源，防止其他硬件因错误的工作电压而损坏。

收到“Power Good”信号后，主板上的芯片组就会开始工作。芯片组会首先复位CPU，然后CPU从固定的地址开始执行整个计算机启动中的第一个指令，运行第一个程序——BIOS/UEFI程序，开始加电自检（POST，Power-On Self Test）。加电自检的任务是检测计算机的硬件是否正常工作，一般先按顺序检查CPU、内存、显卡，再检查其余设备。如果发现任何问题，加电自检会发出错误信号，例如蜂鸣声、错误代码等，提示用户进行修复。如果一切正常，加电自检会将控制权交给下一个程序。

\subsection{硬件初始化和配置}

POST通过后，BIOS/UEFI就会开始初始化硬件。它会为每一个PCIe\footnote{PCIe，全称Peripheral Component Interconnect Express，外设组件互连快速通道，是一种高速串行计算机扩展总线标准，用于连接主板和各种硬件设备，例如显卡、网卡、存储设备等。}设备分配总线编号、内存映射I/O空间\footnote{内存映射I/O（Memory-Mapped I/O，MMIO）是一种将外设设备的寄存器映射到计算机内存地址空间中的技术。这样，CPU可以通过读写内存地址来访问外设设备，而不需要使用专门的I/O指令。}和中断号，确保这些设备都能被操作系统正确识别。在这个时候，主板上的RGB灯、风扇、CPU超频等用户自定义配置也会被加载。

这一阶段的速度主要取决于主板固件的优化程度以及硬件的数量和类型。高端主板通常会有更好的固件优化，因此启动速度更快；而安装了大量硬件设备的计算机则需要更多时间来初始化这些设备，导致启动速度变慢。

\subsection{引导}

硬件准备就绪后，BIOS/UEFI就会开始引导操作系统。它会按照预设的引导顺序，依次检查硬盘、SSD、U盘、光驱等设备，寻找操作系统。传统的引导方式Legacy会读取硬盘首个扇区（MBR，512字节），该山区存放着一段大小为446字节的引导代码；而现代的UEFI引导方式则会读取EFI系统分区中的引导文件（通常是 \texttt{bootx64.efi} ），该文件可以存放在FAT32格式的分区中。

如果找遍整个引导顺序都没有找到操作系统，BIOS/UEFI就会发出错误信号“No bootable device”，随即终止启动过程。如果找到了操作系统，BIOS/UEFI就会将控制权交给引导加载程序（Boot Loader），开始加载操作系统。

引导加载程序又会做两件事：第一，把操作系统内核（如 \texttt{winload.efi} 、 \texttt{vmlinuz} 等）加载到内存中；第二，把启动参数（如启动分区、内核参数等）传递给操作系统内核。引导加载程序的速度主要取决于存储设备的读写速度以及引导加载程序的优化程度。

\subsection{内核加载与驱动初始化}

操作系统内核获得控制权后，会首先初始化CPU调度子系统、内存管理子系统，然后加载硬件抽象层（HAL）和必要的驱动程序。现在屏幕大概率会出现操作系统的Logo或者启动动画。随后，内核会并行地初始化其他设备，速度主要决定于磁盘性能、驱动数量、安全策略等因素。

\subsection{用户空间启动与登录界面}

现在内核初始化完毕了，可以启动第一个用户态\footnote{用户态（User Space）是指操作系统中运行应用程序的环境，与内核态（Kernel Space）相对应。用户态中的程序无法直接访问硬件资源和内核数据结构，而是通过系统调用与内核进行交互。这样可以提高系统的安全性和稳定性，防止用户程序对系统造成破坏。}进程了。在Windows中，第一个用户态进程是 \texttt{smss.exe} ，它负责启动其他系统服务和后台进程；在Linux中，第一个用户态进程通常是 \texttt{systemd} 或者 \texttt{init} 。这些进程会负责挂载系统分区、启动日志服务、启动设备管理器等，这些都被叫做“自动启动项”。

当所有的自启动项都加载完毕，登录界面出现在屏幕上，整个启动流程结束。用户输入密码后，Explorer 或桌面环境继续加载用户级程序，至此计算机完全可用。

\subsection{问题解答}

现在我们就可以解答前面提出的问题了。

\textbf{为什么新旧电脑启动速度差异巨大？}

\begin{enumerate}
  \item 新主板往往用的是UEFI Fast Boot（快速启动）技术，可以跳过一些不必要的硬件检测和初始化步骤，从而大幅提升启动速度；而旧的主板往往是全部检测，耗时长；
  \item 新电脑往往使用SSD作为存储设备，SSD的读写速度和和随机读取延迟都要显著优于传统的机械硬盘，因此引导加载程序和内核加载的速度更快；
  \item 新的操作系统往往是干净的，没有太多的自启动项，因此用户空间启动的速度更快；而旧的操作系统往往安装了大量的软件和服务，导致自启动项过多，拖慢了启动速度。
\end{enumerate}

大家完全不必要担心自己的计算机使用的不是UEFI技术，因为今年已经是2025年了，想找到BIOS主板已经非常困难了——除非你用的电脑是在2018之前买的。那么，如果你的计算机确实是旧的主板或依然缓慢，可以进入BIOS设置（在开机前猛按F2、F8、Del等，具体按键见主板说明书），看看有没有“Fast Boot”选项，启用它就能显著提升启动速度。

至于优化启动项，这个只需要别安装一堆不必要的软件就行了，而且在安装软件的时候注意把“开机启动”选项关掉（例如QQ、微信等）。

\textbf{为什么换了个高端显卡，启动反而变慢？}

实际上只有一点，也就是高端显卡的初始化时间更长。高端显卡往往有更多的功能和复杂的硬件设计，因此需要更多的时间来初始化和配置。此外，高端显卡往往需要加载更多的驱动程序和固件，这也会增加启动时间。如果主板的固件没有针对高端显卡进行优化，启动时间则会显著增加。但随着主板固件的更新和优化，这个问题会逐渐得到解决。

\begin{table}[htbp]
  \centering
  \caption{启动故障排查速查表}
  \begin{tabular}{ccc}
    \toprule
    现象 & 可能原因 \\
    \midrule
    按下电源键风扇不转 & 电源线没插、电源损坏、主板供电线松动 \\
    风扇转但没有画面 & 内存没插好、显卡供电不足、或显示器问题 \\
    主板蜂鸣器响 & 内存条松动、显卡没插好、CPU过热，具体含义见主板说明书 \\
    卡在主板Logo & 硬盘没连接好、引导顺序错误、操作系统损坏 \\
    Windows蓝屏 & 驱动程序冲突、硬件故障、系统文件损坏 \\
    Linux内核恐慌 & 硬件不兼容、驱动程序错误、文件系统损坏 \\
    \bottomrule
  \end{tabular}
\end{table}

了解启动流程后，我们就能有针对性地优化：关闭不必要的启动项、启用 Fast Boot、升级固件、更换更快的 SSD，甚至调整显卡固件设置，都能让“按下电源键到进入桌面”的时间显著缩短。

\section{内存怎么被管理？}

刚刚我们提到，计算机的CPU从内存取指令和数据，执行指令，然后把结果再存回内存。但是现在的问题是：对于一些用户，我们可能会在后台挂着114514个进程，这些进程都需要使用内存。但是这些进程所占用的内存可能远远大于实际物理内存的大小。那么，计算机到底怎么管理内存，使得每个进程都能正常运行？

\subsection{虚拟内存}

计算机使用虚拟地址空间来管理内存。每个进程都有自己的虚拟地址空间，都认为自己是从0号地址开始用内存的。操作系统通过虚拟内存技术，将虚拟地址映射到物理地址。这样，每个进程都可以独立地使用内存，而不需要关心其他进程的内存使用情况。

打个比方：某高度智能运行的图书馆给每一本书贴一个标签，标签上写着书的编号；但是读者不需要管实际上书放在哪里，只需要知道自己的书编号就行了。

\subsection{磁盘交换区}

当物理内存不足时，操作系统会将一些不常用的页面（page）从物理内存（快）中换出到磁盘上的交换区（swap space）（慢）。我们可以理解为，图书馆把常用的书放在书架上，而不常用的书放在仓库里。这样，当需要使用不常用的书时，图书馆可以从仓库中取出书来。

\subsection{页面、页表、缺页异常}

页面是虚拟内存的基本单位，通常是4KB或8KB。操作系统使用页表（page table）来管理虚拟地址和物理地址之间的映射关系。如果我们查到了一个虚拟地址对应的物理地址，但是这个页面不在物理内存中，那么就会发生缺页异常（page fault）。操作系统会捕获这个异常，然后从磁盘上的交换区中加载相应的页面到物理内存中。同样利用图书馆打比方：图书馆有一本书的编号，但是这本书不在书架上，而是在仓库里。图书馆会去仓库里取出这本书，然后放到书架上。

如果最近的书架满了，怎么办呢？一个常见的策略是使用LRU（Least Recently Used）算法，淘汰最近最少使用的页面。也就是图书馆会把最近很久没被借阅的书从书架上拿下来，腾出空间来放新书。

\subsection{内存分配器}

内存分配器（memory allocator）是操作系统或运行时库提供的，用于管理进程的内存分配和释放。常见的内存分配器有 \texttt{malloc} 、 \texttt{free} 等函数。内存分配器会维护一张空闲内存块的列表，当进程请求分配内存时，分配器会从空闲列表中找到合适的内存块，并将其分配给进程。

假如我们在C系语言用了malloc函数分配了许多字节的内存，这时候操作系统\textbf{不会}直接分配物理内存，而是分配虚拟内存。操作系统会在页表中记录这个虚拟地址和物理地址的映射关系。而真正给物理页，是“用到才给”，多数情况下，当我们第一次访问这个虚拟地址时，操作系统会触发缺页异常，然后将对应的物理页加载到内存中；少数情况下（例如堆内存），操作系统会预先分配一些物理页而不是延迟到首次访问才分配。

这也可以解释为什么我malloc了10GB内存但是电脑依然流畅运行：还没真正分配呢。

\subsection{一个例子}

假如，我们打开了微信。这时候，操作系统给微信预留了1GB的虚拟地址空间；但是实际上只先分配很少数的物理内存来加载常用数据，剩下的全在磁盘交换区。然后，假设我们又切换到其他应用程序（例如去B站看视频），这时候B站会获得许多新的物理页，而微信的物理页会被换出去一部分。

现在老板给我发消息了，我打开微信，点击几下，这时操作系统触发一个缺页异常，然后微信数据又被拉回内存。如此反复，整个过程只在数十毫秒内完成，使得我们几乎感觉不到延迟。

因此以后谁再拿“某某手机/某某电脑真好，同时开十个APP也不卡”来宣传产品的时候，你可以跟他讲讲虚拟内存！

\section{怎么压榨CPU的性能？}

我们知道，CPU是计算机的核心部件，负责执行指令和处理数据，其做法是从内存中取指令和数据，执行指令，然后把结果再存回内存。但是，现在的计算机内存的速度已经远远跟不上CPU的速度了。我们怎么才能更进一步地压榨CPU的性能呢？

有时候在做超大矩阵乘法的时候，我们发现仅将循环从按列换成按行，或者从按行换成按列，就能将程序的运行速度提升许多。这又是为什么呢？这就涉及到了CPU的缓存机制。

\subsection{缓存的分级}

CPU的缓存（cache），又叫高速缓存，是一种小容量、高速度的存储器，用于存储经常使用的数据和指令。缓存通常分为三级：L1、L2和L3缓存。
\begin{itemize}
  \item L1缓存：位于CPU内部，速度最快（1纳秒级），但容量最小，通常为32KB或64KB。
  \item L2缓存：位于CPU内部或外部，速度较快（3到5纳秒级），容量较大，通常为256KB或512KB。
  \item L3缓存：位于CPU外部\footnote{现代CPU通常集成在内部做多核共享缓存}，速度较慢（10纳秒级），但容量最大，通常为2MB或更大。
\end{itemize}
再往后就轮到内存了，内存的速度大约是100纳秒级别。我们可以利用小卖部来理解，L1缓存有点像学校每层楼都有的贩卖机，L2有点像每栋楼都有的小超市，L3有点像学校的大超市，而内存有点像学校外面的供货仓库。

\subsection{缓存行和局部性原理}

缓存是以缓存行为单位进行存储的。缓存行（cache line）是缓存中最小的传输单位，通常为64字节，但CPU依然能够按字节寻址。当CPU访问内存时，如果访问的地址在某个缓存行内，那么这个缓存行就会被加载到缓存中。我们可以这么理解：当我们去贩卖机只会买一瓶饮料，但是贩卖机补货的时候是一补补一箱。只要把经常一起用的数据放在连续的一个缓存行上，就能一口气全带走，非常方便。

缓存的局部性原理是指程序在执行过程中，访问数据的地址往往具有一定的规律性。局部性分为时间局部性和空间局部性。时间局部性指的是最近访问的数据很可能会再次被访问；空间局部性指的是如果访问了某个地址，那么很可能会访问相邻的地址。

因此，我们在编写程序时，应该尽量利用局部性原理，将相关的数据放在一起，减少缓存未命中（cache miss）的情况。

\subsection{组相联和标签}

缓存通常采用组相联（set-associative）方式来存储数据。组相联缓存将缓存分为多个组，每个组包含多个缓存行。当CPU访问某个地址时，首先计算出该地址对应的组，然后在该组内查找是否有对应的缓存行（way）。如果有，就命中（hit），否则就未命中（miss），需要从内存中加载数据。

每一个缓存行都会贴两个标签，一个是tag记录该缓存行对应的内存地址的高位部分，另一个是valid位记录该缓存行是否有效。在CPU要读一个地址的时候，CPU会先计算出该地址对应的组，然后在该组内查找是否有有效的缓存行。如果有，就命中；如果没有，就未命中，需要从内存中加载数据。

\subsection{未命中常见工作流程}

当CPU访问的地址不在缓存中时，就会发生读不命中。这时，CPU需要从内存中加载数据到缓存中。加载数据的过程通常分为以下几个步骤：
\begin{enumerate}
  \item L1缓存没有，去L2缓存查找；L2缓存没有，去L3缓存查找；L3缓存没有，去内存查找。
  \item 如果找到了，就将数据加载到L1缓存中，并更新L1缓存的标签和有效位。
  \item 如果L1缓存满了，就需要选择一个缓存行进行替换。通常使用LRU（Least Recently Used）算法来选择最近最少使用的缓存行进行替换。L2和L3缓存也会进行类似的替换操作。
\end{enumerate}

如果CPU试图往缓存中写入数据，而该缓存行已经被其他数据占用，那么就触发了写不命中。一般有一些策略来处理写不命中，例如写回（write-back）和直写（write-through）。写回策略是将数据先写入缓存，等到当缓存行被标记为“脏”时才写回时再写回内存；直写策略是直接将数据写入内存。写分配和不写分配是指在写不命中时，是否将数据加载到缓存中。写分配会将数据加载到缓存中，而不写分配则不会。

\subsection{大矩阵乘法的工作原理}
于是我们讲完了缓存，现在就可以来解释为什么有时候换个循环顺序就能提速一倍了。

一般情况下，一个二维数组，按行扫的时候，相邻的元素在内存连续，64个字节一口气全都搬进L1，命中率非常高；而按列扫的时候，相邻的元素在内存中并不连续，可能需要多次访问L2和L3缓存，命中率就会降低。

另一种方式就是手动对齐数据，例如利用结构体来对齐数据。这样可以防止诸如double等长数据类型被拆成好几个缓存行，手动对齐可以强制把这样的64位数据按进一个缓存行，速度至少翻倍。

简单地说，只要让常用数据挤在同一箱里，就能让小卖部永远有货。

\subsection{流水线}

上述缓存机制虽然显著提升了CPU的性能，但是CPU依然有一个瓶颈：指令执行的速度远远跟不上CPU的时钟频率\footnote{CPU的时钟频率指的是CPU每秒钟能够执行的时钟周期数，通常以GHz为单位表示。现代CPU的时钟频率通常在2GHz到5GHz之间，也就是每秒钟能够执行20亿到50亿个时钟周期。时钟周期是CPU时间的最小单位}。为了进一步提升CPU的性能，现代CPU采用了流水线（pipeline）技术。

这个流水线和工厂内的流水线非常类似。例如汽车组装工厂，现在并不是一辆车组装完了再组装下一辆车，而是把组装过程分成多个阶段，每个阶段由不同的工人负责。这样，当第一辆车进入第二个阶段时，第一辆车的第一个阶段已经完成，第二辆车可以进入第一个阶段进行组装。这样，工厂就能够同时组装多辆车，大大提高了生产效率。在CPU中，我们也是这样，把一个指令的执行过程分成：
\begin{enumerate}
\item \textbf{取指（IF）}：根据 PC 把指令读进指令寄存器；
\item \textbf{译码（ID）}：解析操作码、读寄存器堆拿到操作数；
\item \textbf{执行（EX）}：在 ALU 或地址生成单元里完成运算；
\item \textbf{访存（MEM）}：若是 load/store，访问数据缓存；
\item \textbf{写回（WB）}：把结果写回寄存器堆并更新标志位。
\end{enumerate}
然后和工厂中流水线一样，每一级都让独立的硬件单元完成。理想情况下，当上一条指令进入EX阶段时，下一条指令就跟着进入IF阶段，于是每个时钟周期都能完成一条指令的执行，大大提高了CPU的性能。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{106/pipeline.jpg}
  \caption{一张“臭名昭著”的流水线示意图}
\end{figure}

在实际情况下可能有三类“气泡”会让流水线停顿：
\begin{itemize}
  \item 数据冒险：当后一条指令恰好用到了前一条指令尚未写回的结果时，就会发生数据冒险，一个容易想到的解决方法是插入\textbf{气泡}（stall），让流水线停顿一段时间，直到前一条指令的结果写回。另外的解决方法是\textbf{数据转发}（或\textbf{数据前递}，data forwarding），直接把前一条指令的结果旁路到后一条指令的EX级，避免停顿。
  \item 控制冒险：当遇到分支判断的时候，下一条指令的地址实际上是未知的。这时候也容易想到插入气泡来等待分支结果。为了防止这种情况，现代CPU通常会采用\textbf{分支预测}（branch prediction）技术，猜测下一条指令的地址，并提前加载到流水线中。如果猜测正确，就继续执行；如果猜测错误，就丢弃错误的指令，重新加载正确的指令，这样的代价是大约10到20个时钟周期的猜测惩罚。而怎么猜则是一个技术活，常见的方法有静态预测（例如总是猜测分支不跳转）和动态预测（例如利用历史信息来预测分支行为）。
  \item 结构冒险：当多个指令同时竞争同一个硬件资源时，就会发生结构冒险。例如，如果只有一个乘法器，而两条指令都需要使用乘法器，那么就会发生结构冒险。当然插入气泡也并非不可，而通过多端口寄存器堆、分离的指令和数据缓存等方法也可以缓解结构冒险的问题。
\end{itemize}

要是再往上提升性能，一般有三种手段：超标量（superscalar）、乱序执行（OoO）和超线程（SMT）。超标量指的是每一个周期同时发射多条指令到流水线中执行；乱序执行指的是指令不必严格按照程序顺序执行，而是可以根据数据依赖关系和资源可用性来动态调整执行顺序，只要操作数就绪了这条指令就可以抢跑，最后按指令序号重新提交结果；超线程指的是在流水线里面交替塞两条线程的指令，把闲置端口也利用起来。

以上操作对我们写代码有相当的启示：尽量保持分支可预测（有规律，少跳转），减少数据依赖（多用临时变量，少用全局变量），减少资源竞争（少用全局变量，少用锁），循环体小而整齐（减少指令数，增加指令并行度）。这样就能让流水线吃得饱饱的，性能自然就上去了。例如：
\begin{lstlisting}[language=C]
  int cnt = 0;
  for(int i = 0; i < n; i++)
    if (a[i] > 128)
      ++cnt
\end{lstlisting}
这个实践是不好的，因为数组模式随机，分支不可预测，数据依赖严重。改成下面这样就好多了：
\begin{lstlisting}[language=C]
  int cnt = 0;
  for(int i = 0; i < n; i++) {
    int flag = (a[i] > 128);
    cnt += flag;
  }
\end{lstlisting}
这样就消除了分支，数据依赖也减轻了许多，编译器大概会把上述内容编译成 \texttt{setgt} 和 \texttt{add} 指令，流水线就能更好地并行执行。

\begin{tip}
  当然，根据“不优化”原则我们知道，实际操作中未必需要严格这么写，或者说仅在以下情况差异显著：
\begin{itemize}
  \item 数据量巨大，例如n达到百万级别以上；
  \item 数据内容相当随机地分布，例如a[i]的值均匀分布在0到255之间；
  \item 编译器没有做激进优化，例如开的 \texttt{-O0} 或者 \texttt{-O1} ；
  \item CPU是现代超标量、流水线深度相当大的CPU。
\end{itemize}
反之，当数据量小、大多数数据大于128（分支预测器能学习并预测）、编译器激进优化（“吸氧”甚至“吸臭氧”）、使用SIMD指令等技术时，差异就不明显了。

如希望验证我的上述说法，可以利用 \texttt{perf} 等工具进行性能分析，重点观察 \texttt{branch-misses} 、 \texttt{instructions} 、 \texttt{cycles} 等指标， \texttt{-O2} 和 \texttt{-O0} 的差异也可以对比一下。
\end{tip}

\subsection{现代CPU的架构}

旧时代的CPU一般走的是单核高频路线，这也是非常容易想到的提升性能的方式：把一个核的频率提升到极限，然后让这个核尽可能地快地执行指令。这样做的好处是简单易行，缺点是功耗和发热量都非常高，且单核性能提升空间相当有限。这个路线撞墙的例子就是Intel的NetBurst架构（奔腾4），频率最高能达到3.8GHz，但是单核性能并没有显著提升，反而因为发热量过大而被迫降频。

于是，现代CPU性能渐渐地走向了多核化、并行化的路，性能不仅靠GHz撑着，并行度和专用加速也成为了重要的指标。当下主流芯片把多种计算单元拼成SoC，一般还有大小核之分（big.LITTLE架构），大核负责高性能计算，小核负责低功耗计算，二者协同工作以提升整体性能和能效比。

\begin{enumerate}
\item 性能核（P-core）：乱序、宽发射、高频率，跑串行关键路径；
\item 能效核（E-core）：顺序或窄发射，面积小、功耗低，跑后台线程；
\item 矢量/矩阵单元——SSE/AVX/AVX-512、SVE、AMX，一条指令打 512 bit–2048 bit 的 SIMD，做 dense math；
\item 集成 GPU Or NPU：上千线程级的 SIMT，负责图形与 AI 推理；
\item 片上系统：DDR/LPDDR 控制器、PCIe 5.0、CXL、缓存一致性总线（Ring/Mesh），把 CPU、GPU、加速器、内存、外设粘在一起。
\end{enumerate}

而缓存也从上文所述的经典缓存升级为支持网状多切片、非包容/非排他性、智能预取等特性的现代缓存系统，以适应多核、多线程、高并发的计算需求：

\begin{itemize}
\item 每个 P-core 独享 48 KB L1-I + 32 KB L1-D + 1–2 MB L2；
\item 多核通过 Mesh 节点共享 24–96 MB L3，切片数等于核数，降低热点；
\item 目录式（Directory）或总线嗅探（MESIF）协议保证多核一致性，跨核延迟 30–60 ns。
\end{itemize}

而对于我们开头的“超大矩阵乘法”这种还吃内存带宽的计算任务，现代CPU也有不少提升手段：

\begin{itemize}
\item AVX-512 / AMX：单指令可算 $16\times 64$ 矩阵块，理论算力提升4到8倍；
\item 高带宽内存：笔记本 LPDDR5X 已做到 120 GB/s，服务器 HBM3 突破 1 TB/s；
\item 缓存阻塞（cache blocking）：把超大矩阵切成 L2 能装下的子块（如 $256 \times 256$），再在内层用 SIMD 展开，就能把 100 ns 的内存访问变成 5 ns 的 L2 命中，轻松获得10倍数级提速。
\end{itemize}

所以说，现代的CPU并不是单核跑分的时代，而是多核、矢量、缓存墙协同作战。写程序的时候，只需要让计算靠近数据、并行匹配硬件宽度，就能真正的把晶体管一滴不剩地榨成有效算力。


\section{系统怎么被调用？}

有时候我们电脑死机了，或者程序崩溃了，终端报错“Segmentation Fault”（段错误）。这时候，操作系统到底做了什么？为什么会发生段错误？我们来分析一下“系统调用”就知道了。

\subsection{为什么要有这个系统调用？}

一般情况下，程序运行时仅会访问分配给自身的内存中的数据和指令。如果程序试图访问未分配的内存区域，或者试图修改只读内存区域，就会发生段错误。这是出于安全性和稳定性的考量：操作系统需要确保每个进程都只能访问自己的内存区域，不能访问其他进程的内存区域。这样可以有效防止恶意程序破坏系统的稳定性和安全性。

但是有些情况下，程序确实需要访问一些特殊的内存区域，例如访问硬件设备、操作系统内核等。为了解决这种问题，操作系统提供了系统调用（system call）来处理内存访问。

简单地说，可以把操作系统看成化学实验室管理员，管理危化品。把危化品直接扔给学生非常危险，学生必须先向管理员填表申请，管理员检查后再给学生。填的这个表就是系统调用。

\subsection{系统调用长什么样？}

以Linux为例，一个系统调用往往包括系统调用号（放在RAX）、参数（放在RDI、RSI、RDX等寄存器，包括要干什么、干多少次、怎么干）、触发指令（ \texttt{syscall} ）和返回值（放在RAX）。当程序需要进行系统调用时，会使用 \texttt{syscall} 指令来触发系统调用。系统调用的种类很多，例如read、write、open、close等，每个系统调用都有一个唯一的系统调用号。

以实验室为例，上述填表就包括：编号（系统调用号）、申请的危化品（参数）（包括要什么、要多少、放哪）、申请的指令（ \texttt{syscall} ），以及管理员的批复（返回值）。当学生需要使用危化品时，就会向管理员提交申请，管理员检查后返回批复。

\subsection{系统调用的处理流程}

当程序触发系统调用时，CPU会将当前的执行状态保存到内核栈中，然后切换到内核态（kernel mode）。在内核态下，操作系统会根据系统调用号找到对应的系统调用处理函数，并执行相应的操作。处理完成后，操作系统会将结果返回给用户态（user mode），并恢复之前保存的执行状态。

以 \texttt{printf("Hello")} 为例，这个东西实际上是做了一次系统调用 \texttt{write(1, buf, 5)} 。现在glibc把这玩意塞进寄存器触发syscall指令，然后CPU就切换到内核态。

之后，CPU在内核态办事：检查文件描述符1是否可写，发现可以写，就把Hello这5个字节写入到文件描述符1对应的设备（通常是终端）。

写完后，CPU会将结果（成功写入的字节数，在这里是5）放回RAX寄存器，然后切换回用户态。然后代码就会继续执行了。

\subsection{系统调用的代价与实践尝试}

系统调用虽然显著提升了系统的安全性，但是也带来了巨大的性能损失。因为每次系统调用都需要切换到内核态，这个过程需要保存和恢复CPU的状态，涉及到上下文切换（context switch），会消耗大量的时间，比普通函数调用慢不少——这还是现代CPU的优化结果。因此，系统调用的次数越少，程序的性能就越好。

在代码实践中，我们最简单的优化方式就是尽可能减少系统调用的次数，例如使用缓冲IO或批量读写等。

\section{计算机怎么和外界交互？}

我们日常把U盘插进接口，耳机塞进耳机孔，点击保存按钮把文件“放”进硬盘，仿佛计算机内部与外部世界之间隔着一堵透明的墙，数据像幽灵一样穿墙而过。实际上，这堵墙上有无数扇“暗门”，门后站着一群翻译官、调度员和安检员，它们共同把数据变成电压，把电压再变回数据，还要防止外设把奇怪的东西带进屋里。本节就拆开这堵墙，看看计算机究竟怎么和外界打交道。

\subsection{I/O地址和端口}

CPU 并不直接认识“鼠标”或“打印机”，它只认“地址”。x86 体系把外设抽象成一组寄存器，每个寄存器被分配一个固定或动态的 I/O 地址：早年的键盘控制器占 0x60–0x64，串口 COM1 占 0x3F8–0x3FF。CPU 用 I/O 指令像写信一样把数据塞进这些“信箱”，外设收到信后再回信，完成一次最小交互。

现代计算机中，大部分外设搬进了“内存映射 I/O”（MMIO）：显卡的几百 MB 显存、NVMe 控制器的寄存器，都被映射到一段物理地址。CPU 把它们当成普通内存读写，但背后由芯片组把地址翻译成 PCIe 事务，送到目标设备。于是，同一条 mov 指令，既可能真往内存写数据，也可能改掉了显卡的光栅化参数——地址就是门牌，门牌背后是什么，由主板上的“片选信号”决定。

\subsection{中断与DMA}

如果CPU只能靠轮询I/O地址来和外设交互，那效率会非常低下：CPU每几毫秒就得问一次“鼠标你动没动？”，这样浪费电，速度还慢。为了解决这个问题，计算机引入了中断机制。

在外设完成动作（如鼠标拖动、键盘按键）后，它会在IRQ（中断请求线）上发出一个信号，中断控制器（现代计算机一般叫APIC，高级可编程中断控制器）收到信号后，会通知CPU暂停当前工作，转而执行一个预设的中断处理程序ISR（中断服务例程），把鼠标位移、网卡数据包什么的都统统发送，处理完毕后再返回继续执行之前的任务。这样，CPU就不需要不停地轮询外设，而是等外设主动来“敲门”，大大提高了效率。

但是中断只能通知“有事”，数据还得CPU亲自搬运，而现代SSD、千兆网卡等外设的数据量巨大，CPU搬运数据的效率远远不够。为了解决这个问题，计算机引入了DMA（直接内存访问）技术。DMA会把数据直接从外设搬到内存，CPU只需要在中断处理程序中告诉DMA控制器“把数据搬到哪里”，然后继续干自己的事。这样，CPU就能腾出更多时间处理其他任务，像稳坐钓鱼台的市长；而其他外设，如网卡、显卡、声卡等也能更高效地工作，总线就像城市立交桥一样繁忙而有序。

\subsection{设备驱动}

有了I/O地址、中断和DMA，CPU就能和外设打交道了。但每个外设的工作方式都不一样，CPU怎么知道该怎么和它们交流呢？这就需要设备驱动程序，它们会把数据真正翻译成外设能理解的格式，外设再把数据按人能理解的方式反馈回来。

一般情况下，设备驱动的上层会提供一个统一的抽象，下层则用无数运算把抽象翻译成具体的数据序列。驱动还要负责电源管理，例如笔记本合上盖子的时候，显卡驱动会把显存压进内存，网卡驱动会把PHY（物理层芯片）关掉，节省电量，掀开盖子再原样恢复。一个驱动bug就可能导致系统真正的“睡死过去”，因此社区经常说驱动“占了70\%的内核代码和90\%的稳定性问题”。

\subsection{热插拔、总线枚举}

早年的打印机必须关机才敢插拔，这叫做冷插拔。因为接口没有电气隔离，插拔时会产生电弧，烧坏接口和设备，甚至把主板烧掉。为了解决这个问题，计算机引入了热插拔技术：接口上加了电气隔离电路，插拔时不会产生电弧；操作系统也会动态识别设备的插拔事件，加载或卸载驱动程序。而其背后，是总线枚举协议在默默工作：
\begin{enumerate}
  \item 新设备插入，接口芯片检测到电压变化，向主机发“Present”信号；
  \item 主机收到信号后，在总线上广播“你是什么设备？”，设备则回复自己的ID、类型等信息；
  \item 主机根据设备信息，加载相应的驱动程序，进而分配地址、中断向量、DMA通道等资源。
\end{enumerate}

\subsection{从电信号到文件：以保存U盘为例}

把“保存文件到U盘”拆成时序，就能看到整条交互链：
\begin{enumerate}
\item 用户点“保存”，应用调用  \texttt{write()}  系统调用，内核把用户缓冲区映射到页缓存；
\item 文件系统（FAT32/exFAT）在缓存里分配新簇，修改 FAT 表，生成 SCSI 命令块；
\item 内核 USB 大容量存储驱动把 SCSI 命令封装成 CBW（Command Block Wrapper），交给 xHCI 主机控制器；
\item xHCI 把 CBW 切成微帧，通过差分信号线以 480 Mbps/5 Gbps/10 Gbps 速率差分发送；
\item U 盘主控收到 CBW，把逻辑块地址翻译成 NAND 物理页，拉高就绪线；
\item 主机发起 DMA，把数据突发写入 U 盘内部缓存，U 盘回传 CSW（Command Status Wrapper）表示“写完”；
\item 内核收到 CSW，把页缓存标记为干净，应用弹出“保存成功”。
\end{enumerate}
整条链路跨越用户态、内核态、USB 总线、闪存转换层，七级抽象、十几种协议，却要在几百毫秒内完成，否则用户就会抱怨“卡了”。这不禁让人感叹计算机的精密和复杂，也难怪在物流行业中计算机属于“精密仪器”或“高价值易损货物”。

\subsection{性能与瓶颈——为什么USB3.0跑不满5Gbps？}
\begin{itemize}
\item \textbf{协议开销：} 每 1024 字节有效数据要附带 20~30 字节包头、CRC、链路控制字，实际速率 ≈ 4 Gbps；
\item \textbf{块大小：} FAT32 默认 32 KB 簇，写小文件时主控要做“读-改-写”，带宽骤降；
\item \textbf{CPU 复制：} 若主板 USB 控制器较老，数据必须经 CPU 内存复制，吃掉 10~20\% 核心；
\item \textbf{闪存限速：} 低端 U 盘使用 TLC/QLC，持续写 1 GB 后触发缓存用尽，速度从 100 MB/s 跌到 20 MB/s。
\end{itemize}
所以，接口规格只是“天花板”，真正的地板由最慢的环节——闪存颗粒、文件系统、驱动质量——共同决定。

\subsection{安全关卡——I/O 权限与恶意外设}
并非所有外设都“人畜无害”。BadUSB 攻击把 U 盘固件刷成虚拟键盘，插入后自动输入恶意命令；Thunderbolt 设备通过 DMA 可直接读写整机内存，绕过 CPU 页表保护。
现代操作系统引入多层防护：
\begin{itemize}
\item \textbf{IOMMU（VT-d、AMD-Vi）：} 把外设 DMA 也纳入地址转换，只允许访问内核提前登记的物理区域；
\item \textbf{代码签名：} Windows 要求内核模式驱动提交 EV 证书，无签名驱动默认拒绝加载；
\item \textbf{USB 授权弹窗：} macOS、GNOME 检测到键盘/网卡类设备首次插入时，要求用户物理点击“允许”，阻断自动注入。
\end{itemize}
即便如此，安全与便利仍像跷跷板：完全锁死外设，研究员的示波器、开发板就无法工作；全部放行，又给攻击者留下后门。操作系统每天都在这架跷跷板上找平衡。

\section{计算机如何联网？}

我们打开计算机，插上网线或连接到无线网络（Wi-Fi），计算机就能上网了。这个过程发生了什么呢？计算机是如何通过网络与其他计算机进行通信的？为了回答这些问题，我们需要了解计算机网络的基本原理。本节中，我们会先介绍一些网络的基本概念，然后从本机开始，逐步将网络扩展到局域网、广域网，最终到达互联网。

\subsection{带宽、传输速率、延迟和丢包率}

这四个名词用来衡量网络的性能，是日常生活中最常见的几个名词。

\textbf{带宽}指的是网络的理论最大传输能力，通常以比特每秒（bps）或字节每秒（B/s）来衡量。带宽越大，网络的传输能力就越强。例如，一个带宽为100Mbps的网络\textbf{理论上}可以每秒传输100兆比特的数据（实际可能远低于这个数值）。我们家里通网的时候说的“千兆宽带”指的就是该网络的带宽是1000Mbps=1Gbps，或125MB/s。

而传输速率、延迟、丢包率则用于衡量网络的实际表现。\textbf{传输速率}指的是网络的实际传输速率，单位也是比特每秒（bps）或字节每秒（B/s），往往显著低于带宽。例如，一个带宽为100Mbps的网络可能实际传输速率只有50Mbps或更低。\textbf{延迟}指的是数据从发送方到接收方所需的时间，通常以毫秒（ms）为单位来衡量。延迟越低，网络的响应速度就越快。例如，一个延迟为50ms的网络意味着数据从发送方到接收方需要50毫秒的时间。\textbf{丢包率}指的是在数据传输过程中丢失的数据包的比例，通常以百分比（\%）来衡量。丢包率越低，网络的可靠性就越高。例如，一个丢包率为1\%的网络意味着在每100个数据包中有1个数据包会丢失。延迟、丢包率、传输速率等指标往往会受到网络拥塞、信号干扰、硬件性能等因素的影响。

\begin{example}
  有一辆满载硬盘的卡车从北京开到上海，估计其平均传输速率、延迟和丢包率，并和现在家用网络进行对比。
\end{example}

\begin{answer}
  先估算带宽。国内高速允许最大总重量为49吨（半挂或板车），一辆半挂车空车质量在16吨以上，这里为了方便按19吨计算，因此装载了30吨的硬盘。一块企业级数据盘容量按30TB（$3\times 10^{13}$ B）计算，自重约700克；算上保护盒等，按1kg计算，因此一辆卡车装了$3\times 10^4$块硬盘，总容量为$9\times 10^{17}$ B，或者0.9EB。

  卡车在高速公路上最大时速在100到90千米每小时不等。按90千米每小时计算，北京到上海约1200千米，则行驶时间大概13.3小时，即$4.8\times 10^4$秒，因此传输速率用总数据量除以时间，得到约$1.9\times 10^{13}$B/s，即19TB/s，约合\textbf{152Tbps}。该数字非常惊人，是目前家用网络的近两万倍。

  下面估计延迟。和常规网络按数据包发送的方式不同，卡车运输是整体运输，或者说卡车本身就是一个大“数据包”，因此延迟等于运输时间，也就是从北京到上海的时间，约13.3小时，即\textbf{$4.8\times 10^4$秒}。

  下面估计丢包率。只要这个车没出事故、没被劫持、没整个掉沟里，那么就可以用公路运输HDD货物损失率0.02\%到0.05\%来充当丢包率。另外，如果出事故，则丢包率为100\%，而按照中国相关统计数据，公路运输百万公里事故数约为1起，因此完全可以忽略不计。综上，\textbf{丢包率约为0.05\%}。

  相对的，现在家用网络的传输速率大概在100MB/s到1GB/s之间，延迟大概在10ms到100ms之间，丢包率大概在0.1\%到1\%之间。可以看到，卡车运输的传输速率远远高于家用网络、丢包率显著低于家用网络，能和最优质的光纤媲美，但是延迟则高得离谱，完全无法进行实时操作。另外，卡车运输还受天气、交通等因素影响，稳定性无法和网络传输相提并论。
\end{answer}

在以往的计算机教材中总是出现一句话：“永远不要小看一辆满载磁带的卡车，其带宽远远超过了家庭网络。”看起来现在也差不多，只不过是把磁带换成了硬盘罢了；实际上，即使是2025年，把1EB数据从北京运到上海这个任务，最经济、最快速的方案依然是物流，其速度甚至能把5GB/s的高端光纤专线按在地上摩擦，成本更是低得多；只要不是那么要求时效性，物流依然是传输超大量数据的首选方案。

上述例子也提示我们怎么选择数据传输方式：GB级别的数据，可以使用任意网络途径传输；TB级别的数据，考虑使用专业的数据传输服务；PB级别的数据，考虑走光纤专线；EB级别的数据，则应考虑物流途径。要是数据量更大，那比起你把数据运过去，不如让对面把计算任务运过来。当然，上述数据是对于企业级别的网络而言的，个人用户的网络情况往往更极端，TB级别数据就可以考虑走快递等物流途径了；例如想把某些大文件从大连运送到沈阳，走网络可能需要几天时间才能传输完，而自行开车一天就能送到

\subsection{内网和外网}

我们在日常生活中，常常会听到“内网”和“外网”这两个词。内网是指一个局域网内部的网络，通常用于家庭、学校或者公司等小范围的网络。内网中的计算机可以通过路由器或者交换机等设备连接到外网。外网是指互联网上的网络，通常用于连接不同的局域网和广域网。

内网和外网的IP地址往往是不同的。内网IP地址通常是私有的IP地址，仅在内网中有效（例如每一个地级市都可能有一个“二中”，但是在不同的市称呼“二中”指的不是同一个学校）；而外网IP地址全球唯一，互联网可以访问（例如“东港二中”）。例如大名鼎鼎的 \texttt{8.8.8.8} 是Google的公共DNS服务器的IP地址，它是一个外网IP地址。如果在内网中访问该地址，则可能访问到的不是Google的DNS服务器，而是内网中的某个设备。

\subsection{网络协议}

网络协议是计算机之间进行通信的规则和约定。它定义了计算机如何发送和接收数据，以及如何处理错误和异常等情况。常见的网络协议有TCP/IP、HTTP、FTP等。不同的网络协议适用于不同的应用场景，例如TCP/IP协议适用于可靠的数据传输，而HTTP/HTTPS协议适用于Web应用程序的通信、UDP适用于精度要求不太高的实时通信等。

\subsection{网络设备}

网络设备是用于连接计算机和其他设备的硬件设备。常见的网络设备有路由器、交换机、集线器等。\textbf{路由器}用于连接不同的网络，并且可以根据网络协议进行数据转发；\textbf{交换机}用于在同一局域网内连接多个设备，并且可以根据MAC地址进行数据转发；\textbf{集线器}用于将多个设备连接到同一个网络，但不具备智能转发功能。

\textbf{光网络单元}（光猫，ONU）是用于将数字信号转换为光信号的设备，通常用于连接到光纤。光猫可以将计算机发送的数据转换为模拟信号，并且将接收到的模拟信号转换为数字信号。家用的光纤宽带通常是通过光猫连接到互联网的。

目前家用的路由器往往集成了光猫和交换机的部分功能，因此我们往往不需要像以前一样购买一大堆设备了。

下面，我们将会逐步讲述计算机从开机到联网，乃至访问互联网、数据传输的全过程。

\subsection{网卡上线}

我们开机，电脑POST、初始化硬件、引导操作系统。操作系统走完启动流程，内核把驱动一个个唤醒。轮到网卡的时候，它还在睡觉。网卡驱动这时候会给MAC控制器喂一口“复位”寄存器，然后写EEPROM里面的MAC地址\footnote{MAC地址是网卡的唯一标识符，由制造商在生产时烧录到EEPROM中，全球唯一。}。这时候，网卡才睡醒：它开始“打拍子”，往双绞线\footnote{也就是“网线”，无线网卡则是收发无线信号。}里面打1000BASE-TX\footnote{1000BASE-TX，全称Gigabit Ethernet over Twisted Pair，千兆以太网双绞线传输标准，是一种以太网物理层协议，支持在双绞线上以1Gbps的速率传输数据。它使用4对双绞线进行全双工通信，采用了先进的编码和调制技术，以提高传输效率和抗干扰能力。}的NLP\footnote{NLP，全称Normal Link Pulse，正常链路脉冲，是以太网物理层协议中的一种信号，用于表示网络连接的存在和状态。在100BASE-TX和1000BASE-TX等以太网标准中，NLP信号通过在双绞线上周期性地发送脉冲来维持链路的活跃状态。}信号，并监听对面有没有回音。对面是交换机，如果一切正常会回复FLP\footnote{FLP，全称Fast Link Pulse，快速链路脉冲，是以太网物理层协议中的一种信号，用于表示网络连接的存在和状态。在100BASE-TX和1000BASE-TX等以太网标准中，FLP信号通过在双绞线上周期性地发送脉冲来维持链路的活跃状态。}信号。网卡收到FLP信号后，就知道对面有人了，接下来就可以开始协商速率、双工模式等参数，最终进入“上线”状态，链路灯两栖，网卡向操作系统抛NETDEV\_UP事件，表示“我上线了”。

\subsection{DHCP：办临时身份证（IP）}

现在协议栈还是光秃秃的，只有一个MAC地址，别的什么都没有。于是，网卡构造一个以太网广播包，目的是FF:FF:FF:FF:FF:FF，来源为自己，Ethertype\footnote{Ethertype是以太网帧头中的一个字段，用于标识上层协议类型。它是一个16位的无符号整数，通常以十六进制表示。例如，IPv4的Ethertype为0x0800，IPv6的Ethertype为0x86DD，ARP的Ethertype为0x0806。}为0x0800（IPv4），里面是UDP源端口68、目的端口67的DHCP Discover报文，走UDP\footnote{UDP，全称User Datagram Protocol，用户数据报协议，是一种无连接的传输层协议，用于在计算机网络中传输数据。与TCP（传输控制协议）不同，UDP不提供可靠的数据传输和流量控制，因此适用于对实时性要求较高但对数据完整性要求较低的应用场景，如视频流、在线游戏等。}协议发走，表示“我来了，给我分个IP地址吧”。

交换机收到内容后，这个包就洪泛到所有端口；路由器则把广播改单播，扔给DHCP服务器\footnote{DHCP服务器通常是路由器自带的，也可以是专门的DHCP服务器。}。DHCP服务器收到请求后，查池子，挑一个空闲的IP，回复：192.168.1.123/24，租期86400秒，网关192.168.1.1，DNS 8.8.8.8\footnote{这里的IP地址、网关、DNS服务器地址仅为示例。}。随即终端回Request、服务器回ACK\footnote{ACK，全称Acknowledgment，确认，是计算机网络中的一种控制信息，用于确认数据包的接收和传输。在TCP协议中，ACK是一个重要的标志位，用于表示接收方已经成功接收到发送方发送的数据包，并通知发送方可以继续发送下一个数据包。}，一来一回四个包，俗称DORA（Discover-Offer-Request-Ack）。至此，计算机就有了临时身份证：IP地址、子网掩码、网关、DNS服务器等信息。如果我们抓包的话，会发现这些包的Transaction ID都是一样的，用于标识同一个DHCP会话，像一起开黑的暗号，匹配不上就丢弃掉，防止隔壁老王冒领。

收到这个“临时身份证”后，操作系统就把IP地址、子网掩码、网关等信息写进内核数据结构。

\subsection{ARP：户籍管理人员}

现在计算机有了IP地址，可以和同一子网内的其他计算机通信了。但有了IP是不够的，数据包最终还需要知道发送到哪里，这依赖于MAC地址。协议栈查路由，发现网关192.168.1.1在直连网段，于是构造一个ARP Request广播：“谁有192.168.1.1？告诉192.168.1.123！”

网关会收到该信息，回复这个Unicast\footnote{Unicast，单播，是计算机网络中的一种通信方式，指数据包从一个发送方传输到一个特定的接收方。与广播（Broadcast）和组播（Multicast）不同，单播通信只涉及两个节点，即发送方和接收方。} ARP Reply：“192.168.1.1对应74:ac:5f:xx:xx:xx。”ARP会缓存60秒，防止频繁广播。

ARP还能防止主机的IP冲突：如果两台主机IP相同，会同时回复包：duplicate address detected，提示用户修改IP。

\subsection{路由：熟知路径的快递员}

假如我们ping了8.8.8.8。于是，协议栈就会查最长前缀匹配：
\begin{enumerate}
  \item 8.8.8.8不在192.168.1.0/24网段内，不能直连，走默认路由192.168.1.1，构造以太网帧，目的MAC为网关，源MAC为自己，Ethertype为0x0800（IPv4）；
  \item 交换机查CAM表，发现不认识目的MAC，把帧扔到上联口；
  \item 路由器收到帧，剥掉二层头，查三层转发表，找到下一跳10.0.0.2\footnote{这个IP地址仅为示例。}，TTL减1\footnote{TTL，全称Time To Live，生存时间，是计算机网络中的一个字段，用于限制数据包在网络中的寿命。它是一个8位的无符号整数，表示数据包在网络中可以经过的最大跳数（hop count）。每当数据包经过一个路由器时，TTL值就会减1，当TTL值减到0时，数据包就会被丢弃，以防止数据包在网络中无限循环。}，重新构造以太网帧，目的MAC为下一跳；
  \item 运营商核心继续查BGP路由，AS\_PATH\footnote{AS\_PATH，全称Autonomous System Path，是边界网关协议（BGP）中的一个重要属性，用于描述数据包在自治系统（AS）之间的路径。自治系统是指由一个或多个网络运营商管理的网络集合，具有统一的路由策略和管理权限。}像快递单一样，经4837、15169\footnote{4837是中国电信的AS号，15169是Google的AS号。}等多个AS，最终到达目的地Google DNS服务器。
\end{enumerate}
整个过程涉及多层协议、多级路由器，最终实现了从本地计算机到全球互联网的通信。以上跳跃中，每一跳的TTL都会减去1，如果TTL减到0，数据包就会被丢弃，并发送ICMP\footnote{ICMP，全称Internet Control Message Protocol，互联网控制消息协议，是一种用于在计算机网络中传输控制信息的协议。它是TCP/IP协议族中的一个重要组成部分，主要用于报告网络错误、诊断网络问题以及传输网络状态信息。}的超时消息给源主机，防止数据包在网络中无限循环。这也是Linux中traceroute\footnote{在Windows中叫tracert。}命令的原理：把TTL从1开始逐渐增大，记录每一跳的路由器地址和响应时间，绘制出数据包从源主机到目的主机的路径。

\subsection{NAT：内网到外网的门房}

现在大部分家庭网络都使用私有IP地址（如192.168.x.x、10.x.x.x等），这些地址在互联网中是不可路由的。为了让家庭网络中的计算机能够访问互联网，路由器会使用NAT（网络地址转换）技术，把私有IP地址转换成公共IP地址。当计算机发送数据包到互联网时，路由器会把源IP地址改成自己的公共IP地址，并记录下这个映射关系；当互联网的服务器回复数据包时，路由器会根据映射关系，把目的IP地址改回私有IP地址，然后转发给计算机。

NAT表项默认120秒无流量则过期，防止表项爆满，于是微信等软件就需要发送“心跳包”来维持连接——这就是为什么有时候没有使用某些软件但这些软件却一直在联网。

\subsection{DNS：域名解析翻译官}

光有IP地址可不行，这太难记了：8.8.8.8是Google，那百度的呢？Bing的呢？逐个记忆IP地址太麻烦了，即使是查表也不方便。为了解决这个问题，计算机引入了DNS（域名系统）技术，把域名（www.google.com）映射到IP地址（8.8.8.8）。容易看到，域名实际上就是我们常说的“网址”\footnote{严格来说，网址（URL）包含了域名、路径、查询参数等信息，而域名只是网址的一部分。例如， \texttt{https://www.example.com/path?query=1} 是一个完整的网址，其中 \texttt{www.example.com} 是域名。}。

浏览器里输入www.google.com后，计算机会先查 \texttt{/etc/hosts} 文件，再查本地DNS缓存，如果没有命中，就会向配置的DNS服务器发送DNS查询请求。DNS服务器收到请求后，会查找自己的数据库，如果找到了对应的IP地址，就会把结果返回给计算机；如果没有找到，就会向根DNS服务器、顶级域DNS服务器等递归查询，最终找到结果并返回给计算机。计算机收到结果后，就可以使用这个IP地址与Google服务器进行通信了。

DNS也是非常容易受攻击的暗礁区：DNS劫持、DNS污染等攻击手段可以篡改DNS查询结果，甚至有些情况下还可以劫持NXDOMAIN\footnote{NXDOMAIN，全称Non-Existent Domain，是DNS查询中的一个响应代码，表示所查询的域名不存在。当计算机向DNS服务器发送查询请求时，如果DNS服务器无法找到对应的域名记录，就会返回NXDOMAIN响应，告诉计算机该域名不存在。}，把不存在的域名解析到攻击者指定的IP地址，从而实现钓鱼网站、插广告，于是就有了DoH（DNS over HTTPS）等加密DNS协议，防止中间人篡改DNS查询结果。

\subsection{TCP三次握手：可靠连接的建立}

现在我们有了IP地址，可以和服务器通信了。但走UDP直接发送数据包是不可靠的：数据包可能丢失、乱序、重复等。为了解决这个问题，计算机引入了TCP（传输控制协议）技术，通过三次握手建立可靠的连接。

假如浏览器拿到47.246.24.134（淘宝），发起SYN\footnote{SYN，全称Synchronize，是TCP协议中的一个标志位，用于表示连接请求。当客户端想要与服务器建立TCP连接时，会发送一个带有SYN标志的数据包，表示“我想和你建立连接”。服务器收到这个数据包后，会回复一个带有SYN和ACK标志的数据包，表示“我同意建立连接，并确认你的请求”。最后，客户端再发送一个带有ACK标志的数据包，表示“连接建立成功”。}包，序列号随机x；服务器则回复SYN-ACK包，序列号随机y，确认号x+1；浏览器再回复ACK包，确认号y+1。至此，连接建立完成，双方各自维护源IP、源端口、目的IP、目的端口四元组，以及序列号、滑动窗口、拥塞窗口等状态信息，确保数据传输的可靠性和有序性。

TCP虽然比UDP慢，但它把不可靠的IP变成了可靠的字节流，适合传输文件、网页等需要完整性的应用。

\subsection{HTTP：网页浏览的协议}

现在我们可以和服务器通信了，但数据包还是二进制的，浏览器看不懂。为了解决这个问题，计算机引入了HTTP（超文本传输协议）技术，把数据包格式化成文本，方便浏览器解析和渲染。

假如浏览器访问 \texttt{http://www.example.com} ，首先会发起一个HTTP GET请求，包含请求行、请求头等信息；服务器收到请求后，会返回一个HTTP响应，包含状态行、响应头和响应体等信息。浏览器收到响应后，就可以解析HTML、CSS、JavaScript等内容，渲染出网页。HTTP是无状态的协议，每个请求都是独立的，服务器不会记住之前的请求状态。

\subsection{TLS：加密数据的押运者}

HTTP传输的数据是明文的，容易被窃听和篡改。为了解决这个问题，计算机引入了HTTPS（HTTP安全）技术。该技术通过TLS技术的握手协商加密算法和密钥，确保数据传输的机密性和完整性。

假如浏览器访问 \texttt{https://www.example.com} ，首先会发起TLS握手：客户端发送ClientHello消息，包含支持的TLS版本、加密套件等信息；服务器回复ServerHello消息，选择TLS版本和加密套件，并发送数字证书；客户端验证证书的合法性，然后生成预主密钥，使用服务器的公钥加密后发送给服务器；服务器使用私钥解密，双方根据预主密钥生成对称加密密钥。握手完成后，双方就可以使用对称加密算法（如AES）进行数据传输了。

TLS在性能上比较差，因此也是优化的重点：AES-NI指令集\footnote{AES-NI，全称Advanced Encryption Standard New Instructions，是英特尔公司推出的一组用于加速AES（高级加密标准）算法的指令集扩展。AES-NI指令集包含了一系列新的CPU指令，可以显著提高AES加密和解密的性能，减少加密操作对CPU资源的占用。}、ChaCha20算法\footnote{ChaCha20是一种流密码算法，由Daniel J. Bernstein设计。它基于Salsa20算法，具有更高的安全性和性能。ChaCha20使用256位密钥和64位随机数，能够在软件实现中提供高效的加密和解密操作。由于其优越的性能和安全性，ChaCha20被广泛应用于各种加密协议和应用程序中，如TLS 1.3、SSH等。}、0-RTT握手\footnote{0-RTT，全称Zero Round Trip Time，是TLS 1.3协议中的一种优化技术，旨在减少连接建立的延迟。在传统的TLS握手过程中，客户端和服务器需要进行多次往返通信（RTT）来协商加密参数和密钥，这会导致较高的延迟。0-RTT技术允许客户端在第一次握手时发送加密数据，而不需要等待服务器的响应，从而实现“零往返时间”的连接建立。这种技术特别适用于需要快速连接建立的应用场景，如移动设备和实时通信等。}等技术都在不断提升TLS的性能。

\subsection{HTTP/2和QUIC：更快的数据传输}

HTTP/1.1只有一条TCP连接，只能串行传输数据，效率低下。为了解决这个问题，计算机引入了HTTP/2技术，通过多路复用、头部压缩等技术，提高数据传输的效率。HTTP/2使用二进制帧格式，把多个请求和响应复用在一条TCP连接上，减少了连接建立和关闭的开销。但是该技术仍然依赖TCP，受到TCP头阻塞问题\footnote{TCP头阻塞问题是指在TCP协议中，由于数据包的顺序传输和确认机制，当一个数据包丢失或延迟时，后续的数据包即使已经到达接收端，也无法被处理，导致整个连接的传输效率下降。这种现象被称为“头阻塞”，因为它阻塞了后续数据包的处理。}的影响，丢一个包则全车等人。

于是，Google把TCP+TLS搬进UDP，改名为QUIC（快速UDP互联网连接），并在HTTP/3中使用QUIC作为传输协议。QUIC通过内置的拥塞控制和多路复用，进一步提升了数据传输的效率和可靠性。QUIC还支持0-RTT连接建立，减少了握手延迟，提高了用户体验。

\subsection{Wi-Fi：空气中的以太网}

上述内容其实基本上全都是有线网络的原理。而无线网络则是“空气中的以太网”。STA（Station）扫描AP\footnote{AP，全称Access Point，接入点，是无线局域网（WLAN）中的一种网络设备，用于连接无线设备（如笔记本电脑、智能手机等）到有线网络（如以太网）。AP通常通过有线连接到路由器或交换机，然后通过无线信号与无线设备进行通信。AP可以提供无线覆盖范围，允许多个无线设备同时连接到网络，实现无线数据传输和互联网访问。}的SSID（分主动和被动两种方式），找到SSID后发起关联请求，AP回复关联响应，双方建立连接。然后，STA发起DHCP请求，获取IP地址等信息，最终实现联网。

现代Wi-Fi有不同的频段，主要频段有2.4GHz和5GHz，最新的Wi-Fi 6E还引入了6GHz频段。2.4GHz仅有3不重叠的信道，邻居家路由器也能像广场舞大妈一样抢占地盘，只能让“城管”RTS/CTS来协调，但吞吐率低；5GHz有更多信道，干扰少，速度更快，但穿墙能力差。目前所见的6GHz路由器还不多，速度更快但穿墙能力更差。

\begin{tip}
  出现上述现象的原因是，Wi-Fi本质上还是电磁波，频率越高，波长越短，穿透能力越差。而频率更短的情况下，单位时间内能传输的数据量就越大，因此速度更快。

  我们发现，Wi-Fi所用电磁波的频率随着技术发展不断提高。但是6GHz波段距离可见光（频率约380THz到750THz）还有一定距离。因此我们设想：将来有一天可不可能\textbf{用可见光}来做无线通信呢？答案是肯定的，已经有相关的研究和应用，但眼见的未来暂时还没办法见到可见光在传输方面的大规模应用，目前仅有光纤通信使用可见光波段来传输数据（但仍局限于红光和近红外光波段）。

  但是让人哭笑不得的是，很久以前的笔记本电脑上确实存在一个红外端口，用来传输数据的，速度还挺慢的，现在都没有了。至于为什么当时红外光传输速度那么慢……这里卖个关子，请同学们自行查阅相关资料吧。
\end{tip}

\subsection{CDN：网络上的缓存}

现代互联网中，CDN（内容分发网络）技术被广泛应用于加速网页加载和视频播放。CDN通过在全球范围内部署大量的边缘服务器，把热门内容缓存到离用户更近的位置，减少了数据传输的距离和延迟，提高了用户体验。

假如我们使用淘宝，图片在上海被北京用户请求，如果每一次都走杭州那延迟就很高了。CDN则会把图片缓存到北京的边缘服务器（例如天津），用户DNS解析会解析到天津去，这样用户请求图片时，就能直接从天津的边缘服务器获取，显著提升加载速度，这显然是模仿了CPU“缓存”的思路——但非常大规模的缓存，也很好用。

\subsection{代理和VPN：外网到内网的桥梁}

内网像一座有围墙的城堡。很多数据库、OA、邮件服务器等重要资源都部署在内网中，也只认内网IP地址。但是需求是多样的：有时候我们在校外打比赛或休假，但还要访问北京大学的内网资源，这怎么办？——请一位门房代为传话即可，这就是代理和VPN的作用。

代理是一种中间人服务，用户把请求发送给代理服务器，由代理服务器代为访问目标资源，然后把结果返回给用户。或者说，我们可以把代理看成雇了一个人帮我们办事：我们把想要的东西告诉他，他去城堡里取回来，然后交给我们。这样，用户就可以通过代理服务器访问内网资源，而不需要直接连接到内网中。常见的代理协议有HTTP代理、SOCKS代理等。北京大学早年确实提供过这个服务，但现在已经被VPN取代了。

而VPN（虚拟专用网络）则是通过加密隧道，把用户的计算机直接连接到内网中，仿佛用户的计算机就在内网中一样。这样，用户就可以直接访问内网资源，而不需要通过代理服务器。VPN通常使用IPSec、OpenVPN等协议来实现加密和认证，确保数据传输的安全性和完整性。这就像是用一条超超超长的虚拟网线，把用户的计算机直接连接到内网中，帮你虚拟“回校”。

举例：北京大学VPN。

\begin{thinking}
  \begin{enumerate}
    \item 验证：浮点误差能累积到金融级别不可接受。试着使用不同的方法累加（不是乘法）1亿个0.01： \texttt{long double} 、 \texttt{double} 、 \texttt{float} 、Kahan求和、 \texttt{libmpdec} 高精度库等，看看结果有什么差异。同时，使用 \texttt{perf stat} 等工具，看看不同方法的性能差异，量化误差和性能的权衡。最终，试着用文中提到的 \texttt{int} 方法来实现货币运算，看看结果和性能如何。
    \item 可执行文件究竟长什么样？GCC编译出的 \texttt{*.o} 和ELF文件究竟有什么差异？试着把这两个文件用十六进制编辑器打开，看看里面都有什么内容；标出ELF文件中的 \texttt{e\_entry} 、 \texttt{program header} 、 \texttt{section header} 等字段，并解释它们的作用。
    \item 优化究竟是怎么暴露未定义行为的？试着故意写一段有整数溢出的看似无害的UB代码，然后使用GCC编译器在不同的优化等级下编译，看看汇编码有什么差异（提示：用 \texttt{diff} 命令比较）。接着，试着打开UBSan运行程序，看看会发生什么。
    \item 虚拟内存真的是免费午餐吗？试着验证一下，当物理内存不足时，操作系统会将哪些页面换出到磁盘交换区？试着写一个程序，分配大量内存（例如 \texttt{malloc 10GB} ）但不访问，观察VmSize和VmRSS的变化；然后，访问这些内存，观察VmSize和VmRSS的变化、记录SIGBUS或者真实缺页的次数，从而理解什么是overcommit（过度承诺）和OOM Killer，以及“用到才给”的含义。
    \item LRU真的永远是最优的吗？试着写一个程序，访问一个大数组（例如100MB），但是访问顺序是随机的，观察缓存命中率和程序运行时间。然后，试着改变访问顺序，例如按行访问、按列访问、斐波那契访问等，观察缓存命中率和程序运行时间的变化，从而理解局部性原理。另一方面，你能写一个程序，使得FIFO比LRU更优吗？
    \item 一条syscall指令事实上产生了多大的性能开销？试着写一个程序，频繁调用一个简单的系统调用（例如 \texttt{getpid()} ），然后使用 \texttt{bpftrace} 或者 \texttt{perf} 等工具，测量系统调用的延迟和CPU周期数，从而量化系统调用的开销。然后，试着将多个系统调用合并为一个系统调用（例如 \texttt{io\_uring} ），观察性能的提升，从而理解减少系统调用次数的重要性。
  \end{enumerate}
\end{thinking}
